{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "abcd_experiments.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPr/pOhlxFTlNlPgra9OVZE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ryan-Qiyu-Jiang/abcd/blob/main/abcd_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJkMKaEYjPhd"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Ryan-Qiyu-Jiang/rloss.git\n",
        "%cd /content/rloss/\n",
        "!git checkout color_query"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/rloss/data/VOC2012/\n",
        "!./fetchVOC2012.sh\n",
        "%cd /content/rloss/data/pascal_scribble/\n",
        "! ./fetchPascalScribble.sh"
      ],
      "metadata": {
        "id": "x4-ND59tjZ4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq tensorboardX\n",
        "!pip install -qqq wandb\n",
        "!pip install -qqq pytorch-lightning"
      ],
      "metadata": {
        "id": "ovWMQ0j9jgy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/rloss/pytorch/pytorch_deeplab_v3_plus"
      ],
      "metadata": {
        "id": "J8WVxn2-1KZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from mypath import Path\n",
        "from dataloaders import make_data_loader\n",
        "from dataloaders.custom_transforms import denormalizeimage\n",
        "from modeling.sync_batchnorm.replicate import patch_replication_callback\n",
        "from modeling.deeplab import *\n",
        "from utils.loss import SegmentationLosses\n",
        "from utils.calculate_weights import calculate_weigths_labels\n",
        "from utils.lr_scheduler import LR_Scheduler\n",
        "from utils.saver import Saver\n",
        "from utils.summaries import TensorboardSummary\n",
        "from utils.metrics import Evaluator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "from torch.nn import functional as F\n",
        "import pytorch_lightning as pl\n",
        "from torch import nn\n",
        "import torch\n",
        "from argparse import Namespace\n",
        "from dataloaders.utils import decode_seg_map_sequence\n",
        "\n",
        "from color_query import BaseModel, get_args, SingleDataset, RepeatDataset\n",
        "\n",
        "segmentation_classes = [\n",
        "    'background','aeroplane','bicycle','bird','boat','bottle',\n",
        "    'bus','car','cat','chair','cow','diningtable','dog','horse',\n",
        "    'motorbike','person','pottedplant','sheep','sofa','train','tvmonitor'\n",
        "]\n",
        "\n",
        "def labels():\n",
        "  l = {}\n",
        "  for i, label in enumerate(segmentation_classes):\n",
        "    l[i] = label\n",
        "  return l\n",
        "\n",
        "def wb_mask(bg_img, pred_mask, true_mask):\n",
        "  return wandb.Image(bg_img, masks={\n",
        "    \"prediction\" : {\"mask_data\" : pred_mask, \"class_labels\" : labels()},\n",
        "    \"ground truth\" : {\"mask_data\" : true_mask, \"class_labels\" : labels()}})"
      ],
      "metadata": {
        "id": "O5uQZJxtrSoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "from dataloaders import make_data_loader\n",
        "\n",
        "args_dict = get_args()\n",
        "args_dict['cuda'] = True\n",
        "args_dict['checkname'] = 'ignore'\n",
        "args_dict['epochs'] = 1\n",
        "args_dict['shuffle'] = False # True for real training\n",
        "args_dict['batch_size'] = 10\n",
        "args_dict['lr'] = 1e-3\n",
        "args_dict['full_gt'] = False # True for gt\n",
        "args_dict['limit_dataset'] = False\n",
        "# args_dict['rloss_scale'] = 1\n",
        "args = Namespace(**args_dict)\n",
        "\n",
        "kwargs = {'num_workers': 6, 'pin_memory': True}\n",
        "train_loader, val_loader, test_loader, nclass = make_data_loader(args, **kwargs)"
      ],
      "metadata": {
        "id": "XdbQ5byZrK2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "batch = iter(train_loader).next()\n",
        "batch_sample = {k:v for k,v in batch.items()}\n",
        "single_dataset = RepeatDataset(batch_sample,  100*10)\n",
        "single_train_loader = DataLoader(single_dataset, batch_size=10, shuffle=True, num_workers=4)\n",
        "single_val_loader = DataLoader(single_dataset, batch_size=10, shuffle=False, num_workers=4)\n",
        "\n",
        "plt.imshow(batch['image'][0].numpy().transpose(1,2,0));"
      ],
      "metadata": {
        "id": "KIeBG7-htBlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "eOxoX-Xb03LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "from utils.metrics import Evaluator\n",
        "\n",
        "def to_img(img_tensor, **kwargs):\n",
        "  if img_tensor.size(0) == 3:\n",
        "    img_tensor = img_tensor.permute(1, 2, 0) # 3 h w -> h w 3\n",
        "  return wandb.Image((img_tensor.cpu().detach().numpy() * 255).astype(np.uint8), **kwargs)\n",
        "log_num = 0\n",
        "\n",
        "# TODO: Make this a layer in a sequence of decoding layers\n",
        "class ColorDecoder(nn.Module):\n",
        "  def __init__(self, num_classes=21, feature_dim=256):\n",
        "    super().__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.feature_dim = feature_dim\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    self.coarse_cls = nn.Conv2d(feature_dim, num_classes, kernel_size=1, stride=1)\n",
        "    \n",
        "  def log(self, d, commit=False):\n",
        "    if log_num % 10 == 0:\n",
        "      wandb.log(d, commit=commit)\n",
        "\n",
        "  def forward(self, feature_map, x, low=None):\n",
        "    image = x\n",
        "    if low is not None:\n",
        "      low = F.interpolate(low, size=image.size()[2:], mode='bilinear', align_corners=True) # bs, num_channels, h, w\n",
        "      x = torch.concat([x, low], dim=1) # bs, (num_channels+3 = d), h, w\n",
        "    \n",
        "    x_dim = x.size(1)\n",
        "    logits_map = self.coarse_cls(feature_map) # c\n",
        "    coarse_segments = self.softmax(logits_map) # bs, num_classes, h, w,  dim(x) = bs, h, w, d\n",
        "    coarse_segments = F.interpolate(coarse_segments, size=image.size()[2:], mode='bilinear', align_corners=True) # s, dim(s) = bs, num_classes, h, w\n",
        "    # sanity check\n",
        "    with torch.no_grad():\n",
        "      mask = torch.max(coarse_segments[:1],1)[1].detach()\n",
        "      mask = wandb.Image(image[0].cpu().numpy().transpose([1,2,0]), masks={\n",
        "        \"prediction\" : {\"mask_data\" : mask[0].cpu().numpy(), \"class_labels\" : labels()}})\n",
        "      self.log({'debug/coarse_segments': mask}, commit=False)\n",
        "    \n",
        "    image_segments_masked =  [  x * coarse_segments[:,i].unsqueeze(1).expand(-1,x_dim,-1,-1) for i in range(self.num_classes) ] # num_classes x (bs, d, h, w)\n",
        "    q = [ torch.mean(s, dim=(2,3)) for s in image_segments_masked ] # mean color of the segment, num_classes x (bs, d)\n",
        "    \n",
        "    # # sanity check\n",
        "    # with torch.no_grad():\n",
        "    #   self.log({'debug/query': [to_img(q[i][0][:3].unsqueeze(0).unsqueeze(0).expand(50, 50,-1)/coarse_segments[0,i].sum()*(image.size(-1)**2), caption=segmentation_classes[i]) for i in range(self.num_classes)]}, commit=False)\n",
        "\n",
        "    attn_maps = [ torch.sum(x * q[i].unsqueeze(-1).unsqueeze(-1).expand(-1, -1, x.size(2), x.size(3)), dim=1) for i in range(self.num_classes) ] # num_classes x  (bs, h, w)\n",
        "    # # sanity check\n",
        "    # with torch.no_grad():\n",
        "    #   self.log({'debug/attn_maps': [to_img(attn_maps[i][0], caption=segmentation_classes[i]) for i in range(self.num_classes)] }, commit=False)\n",
        "\n",
        "    segments_by_color = torch.cat([a.unsqueeze(1) for a in attn_maps],  dim=1) # bs, num_classes, h, w\n",
        "    # sanity check\n",
        "    with torch.no_grad():\n",
        "      mask = torch.max(segments_by_color[:1],1)[1].detach()\n",
        "      mask = wandb.Image(image[0].cpu().numpy().transpose([1,2,0]), masks={\n",
        "        \"prediction\" : {\"mask_data\" : mask[0].cpu().numpy(), \"class_labels\" : labels()}})\n",
        "      self.log({'debug/finer_segments': mask}, commit=False)\n",
        "\n",
        "    global log_num\n",
        "    log_num += 1\n",
        "    return segments_by_color\n",
        "\n",
        "\n",
        "class DeepLabEncoder(nn.Module):\n",
        "  def __init__(self, hparams):\n",
        "    super().__init__()\n",
        "    self.hparams = hparams\n",
        "    model = DeepLab(num_classes=self.hparams.nclass,\n",
        "                                backbone=self.hparams.backbone,\n",
        "                                output_stride=self.hparams.out_stride,\n",
        "                                sync_bn=self.hparams.sync_bn,\n",
        "                                freeze_bn=self.hparams.freeze_bn)\n",
        "    self.encoder = model.backbone\n",
        "    self.aspp = model.aspp\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, low_level = self.encoder(x)\n",
        "    return self.aspp(x), low_level\n",
        "\n",
        "\n",
        "from modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_classes, low_level_inplanes=21, BatchNorm=nn.BatchNorm2d):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(low_level_inplanes, 48, 1, bias=False)\n",
        "        self.bn1 = BatchNorm(48)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.last_conv = nn.Sequential(nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                                       BatchNorm(256),\n",
        "                                       nn.ReLU(),\n",
        "                                       nn.Dropout(0.5),\n",
        "                                       nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                                       BatchNorm(256),\n",
        "                                       nn.ReLU(),\n",
        "                                       nn.Dropout(0.1),\n",
        "                                       nn.Conv2d(256, num_classes, kernel_size=1, stride=1))\n",
        "        self._init_weight()\n",
        "\n",
        "\n",
        "    def forward(self, x, low_level_feat):\n",
        "        low_level_feat = self.conv1(low_level_feat)\n",
        "        low_level_feat = self.bn1(low_level_feat)\n",
        "        low_level_feat = self.relu(low_level_feat)\n",
        "\n",
        "        x = F.interpolate(x, size=low_level_feat.size()[2:], mode='bilinear', align_corners=True)\n",
        "        x = torch.cat((x, low_level_feat), dim=1)\n",
        "        x = self.last_conv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _init_weight(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                torch.nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, SynchronizedBatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "class ColorModel(BaseModel):\n",
        "    def __init__(self, hparams, encoder=None):\n",
        "        super().__init__(hparams)\n",
        "        if encoder is None:\n",
        "          model = DeepLab(num_classes=self.hparams.nclass,\n",
        "                              backbone=self.hparams.backbone,\n",
        "                              output_stride=self.hparams.out_stride,\n",
        "                              sync_bn=self.hparams.sync_bn,\n",
        "                              freeze_bn=self.hparams.freeze_bn)\n",
        "          encoder = model.backbone\n",
        "        self.encoder = encoder\n",
        "        self.decoder = ColorDecoder(num_classes=self.hparams.nclass, feature_dim=256) # resnet feature map dim 320, aspp=256\n",
        "        \n",
        "    def forward(self, x):\n",
        "      feature_map, low_level_feats = self.encoder(x)\n",
        "      return self.decoder(feature_map, x, low=low_level_feats)\n",
        "\n",
        "    def get_loss(self, batch, batch_idx):\n",
        "            i = batch_idx\n",
        "            epoch = self.current_epoch\n",
        "            image, target = batch['image'], batch['label']\n",
        "            target[target==254]=255\n",
        "            self.scheduler(self.optimizer, i, epoch, self.best_pred)\n",
        "            output = self.forward(image)\n",
        "            celoss = self.criterion(output, target.long())\n",
        "\n",
        "            x, _ = self.encoder(image)\n",
        "            coarse_output = self.decoder.coarse_cls(x)\n",
        "            coarse_output = F.interpolate(coarse_output, size=image.size()[2:], mode='bilinear', align_corners=True)\n",
        "            coarse_celoss = self.criterion(coarse_output, target.long())\n",
        "\n",
        "            self.log('train/ce', celoss.item())\n",
        "            self.log('train/course_ce', coarse_celoss.item())\n",
        "            return celoss + coarse_celoss*0.1\n",
        "    \n",
        "    def get_loss_val(self, batch, batch_idx):\n",
        "            # import pdb;pdb.set_trace()\n",
        "            image, target = batch['image'], batch['label']\n",
        "            target[target==254]=255\n",
        "            i= batch_idx % len(batch['image'])\n",
        "            output = self.forward(image)\n",
        "            celoss = self.criterion(output, target.long())\n",
        "            mask = torch.max(output[i].unsqueeze(0),1)[1].detach()\n",
        "            self.val_img_logs += [wb_mask(image[i].cpu().numpy().transpose([1,2,0]), mask[0].cpu().numpy(), target[i].cpu().numpy())]\n",
        "\n",
        "            x, _ = self.encoder(image)\n",
        "            coarse_output = self.decoder.coarse_cls(x)\n",
        "            coarse_output = F.interpolate(coarse_output, size=image.size()[2:], mode='bilinear', align_corners=True)\n",
        "            mask = torch.max(coarse_output[i].unsqueeze(0),1)[1].detach()\n",
        "            self.val_img_logs += [wb_mask(image[i].cpu().numpy().transpose([1,2,0]), mask[0].cpu().numpy(), target[i].cpu().numpy())]\n",
        "\n",
        "            pred = output.data.cpu().numpy()\n",
        "            pred = np.argmax(pred, axis=1)\n",
        "            target = target.cpu().numpy()\n",
        "            self.evaluator.add_batch(target, pred)\n",
        "            result = {\n",
        "              'ce_loss': celoss\n",
        "            }\n",
        "            return result\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        train_params = [{'params': self.get_1x_lr_params(), 'lr': self.hparams.lr},\n",
        "                        {'params': self.get_10x_lr_params(), 'lr': self.hparams.lr * 10}]\n",
        "        self.optimizer = torch.optim.SGD(train_params, momentum=self.hparams.momentum, \n",
        "                                                weight_decay=self.hparams.weight_decay, \n",
        "                                                nesterov=self.hparams.nesterov)\n",
        "        self.scheduler = LR_Scheduler(self.hparams.lr_scheduler, self.hparams.lr,\n",
        "                                            self.hparams.epochs, self.hparams.num_img_tr)\n",
        "        return self.optimizer\n",
        "\n",
        "    def get_1x_lr_params(self):\n",
        "        modules = [self.encoder.encoder]\n",
        "        for i in range(len(modules)):\n",
        "            for m in modules[i].named_modules():\n",
        "                if isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) \\\n",
        "                        or isinstance(m[1], nn.BatchNorm2d):\n",
        "                    for p in m[1].parameters():\n",
        "                        if p.requires_grad:\n",
        "                            yield p\n",
        "\n",
        "    def get_10x_lr_params(self):\n",
        "        modules = [self.decoder, self.encoder.aspp]\n",
        "        for i in range(len(modules)):\n",
        "            for m in modules[i].named_modules():\n",
        "                if isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) \\\n",
        "                        or isinstance(m[1], nn.BatchNorm2d):\n",
        "                    for p in m[1].parameters():\n",
        "                        if p.requires_grad:\n",
        "                            yield p\n"
      ],
      "metadata": {
        "id": "H_3wfwxWz0Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegModelDebug(BaseModel):\n",
        "    def __init__(self, hparams, model=None):\n",
        "        super().__init__(hparams)\n",
        "        self.model = DeepLab(num_classes=self.hparams.nclass,\n",
        "                            backbone=self.hparams.backbone,\n",
        "                            output_stride=self.hparams.out_stride,\n",
        "                            sync_bn=self.hparams.sync_bn,\n",
        "                            freeze_bn=self.hparams.freeze_bn)\n",
        "        self.coarse_cls = nn.Conv2d(256, self.hparams.nclass, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.model(x)\n",
        "    \n",
        "    def get_loss(self, batch, batch_idx):\n",
        "            i = batch_idx\n",
        "            epoch = self.current_epoch\n",
        "            image, target = batch['image'], batch['label']\n",
        "            target[target==254]=255\n",
        "            self.scheduler(self.optimizer, i, epoch, self.best_pred)\n",
        "            # self.optimizer.zero_grad()\n",
        "            output = self.forward(image)\n",
        "            celoss = self.criterion(output, target.long())\n",
        "\n",
        "            x, low = self.model.backbone(image)\n",
        "            x = self.model.aspp(x)\n",
        "            coarse_output = self.coarse_cls(x)\n",
        "            coarse_output = F.interpolate(coarse_output, size=image.size()[2:], mode='bilinear', align_corners=True)\n",
        "            coarse_celoss = self.criterion(coarse_output, target.long())\n",
        "            self.log('train/ce', celoss.item())\n",
        "            self.log('train/course_ce', coarse_celoss.item())\n",
        "            return celoss + coarse_celoss*0.1\n",
        "    \n",
        "    def get_loss_val(self, batch, batch_idx):\n",
        "            image, target = batch['image'], batch['label']\n",
        "            target[target==254]=255\n",
        "            i= batch_idx % len(batch['image'])\n",
        "            output = self.forward(image)\n",
        "            celoss = self.criterion(output, target.long())\n",
        "            mask = torch.max(output[i].unsqueeze(0),1)[1].detach()\n",
        "            self.val_img_logs += [wb_mask(image[i].cpu().numpy().transpose([1,2,0]), mask[0].cpu().numpy(), target[i].cpu().numpy())]\n",
        "\n",
        "            x, low = self.model.backbone(image) \n",
        "            x = self.model.aspp(x)\n",
        "            coarse_output = self.coarse_cls(x)\n",
        "            coarse_output = F.interpolate(coarse_output, size=image.size()[2:], mode='bilinear', align_corners=True)\n",
        "            mask = torch.max(coarse_output[i].unsqueeze(0),1)[1].detach()\n",
        "            self.val_img_logs += [wb_mask(image[i].cpu().numpy().transpose([1,2,0]), mask[0].cpu().numpy(), target[i].cpu().numpy())]\n",
        "\n",
        "            pred = output.data.cpu().numpy()\n",
        "            pred = np.argmax(pred, axis=1)\n",
        "            target = target.cpu().numpy()\n",
        "            self.evaluator.add_batch(target, pred)\n",
        "            result = {\n",
        "              'ce_loss': celoss\n",
        "            }\n",
        "            return result\n",
        "\n",
        "    def get_10x_lr_params(self):\n",
        "        modules = [self.model.decoder, self.model.aspp, self.coarse_cls]\n",
        "        for i in range(len(modules)):\n",
        "            for m in modules[i].named_modules():\n",
        "                if isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) \\\n",
        "                        or isinstance(m[1], nn.BatchNorm2d):\n",
        "                    for p in m[1].parameters():\n",
        "                        if p.requires_grad:\n",
        "                            yield p\n",
        "\n",
        "\n",
        "class SegModel(BaseModel):\n",
        "    def __init__(self, hparams, model=None):\n",
        "        super().__init__(hparams)\n",
        "        if model is None:\n",
        "          model = DeepLab(num_classes=self.hparams.nclass,\n",
        "                            backbone=self.hparams.backbone,\n",
        "                            output_stride=self.hparams.out_stride,\n",
        "                            sync_bn=self.hparams.sync_bn,\n",
        "                            freeze_bn=self.hparams.freeze_bn)\n",
        "        self.model = model\n",
        "        \n",
        "    def forward(self, x):\n",
        "      return self.model(x)\n",
        "\n",
        "    def get_loss(self, batch, batch_idx):\n",
        "        i = batch_idx\n",
        "        epoch = self.current_epoch\n",
        "        image, target = batch['image'], batch['label']\n",
        "        target[target==254]=255\n",
        "        self.scheduler(self.optimizer, i, epoch, self.best_pred)\n",
        "        # self.optimizer.zero_grad()\n",
        "        output = self.forward(image)\n",
        "        celoss = self.criterion(output, target.long())\n",
        "        self.log('train/ce', celoss.item())\n",
        "        return celoss"
      ],
      "metadata": {
        "id": "Jtzc9pqTWna7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.batch_size = 10\n",
        "args.limit_dataset = False\n",
        "args.nclass = nclass\n",
        "args.lr = 0.01\n",
        "args.epochs = 1\n",
        "# train_loader, val_loader, test_loader, nclass = make_data_loader(args, **kwargs)\n",
        "train_loader, val_loader = single_train_loader, single_val_loader\n",
        "\n",
        "args.num_img_tr=len(train_loader)\n",
        "# model = ColorModel(args, encoder=DeepLabEncoder(args))\n",
        "model = SegModelDebug(args)\n",
        "# model.configure_optimizers()\n",
        "# for param in model.encoder.model.backbone.parameters():\n",
        "#   param.requires_grad = False\n",
        "\n",
        "wandb_logger = WandbLogger(project='Color-Query', name='deeplabv3+_seeds') # prototype_aspp_seeds  deeplabv3+_color-low-feats\n",
        "\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=1, logger=wandb_logger, log_every_n_steps=10, num_sanity_val_steps=0, progress_bar_refresh_rate=0, accumulate_grad_batches=1)\n",
        "results = trainer.fit(model, train_loader, val_loader)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "NfYLmnEQz8ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gc\n",
        "# for obj in gc.get_objects():\n",
        "#     try:\n",
        "#         if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
        "#             print(type(obj), obj.size())\n",
        "#     except:\n",
        "#         pass"
      ],
      "metadata": {
        "id": "TUQSe-yj1Nzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dXZDsraULULp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}